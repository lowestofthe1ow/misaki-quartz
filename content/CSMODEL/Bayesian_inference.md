## Frequentist statistics

- **Frequentist** approaches to statistics test whether an event will occur or not based on many repetitions of some experiment.
- The result is therefore dependent on the number of times the experiment is repeated.
<!--ID: 1750164579889-->

## Conditional probability

- **Conditional probability** is the probability of an event $A$ *given that* an event $B$ has already happened. $$P(A | B) = \frac{P(A \cap B)}{P(B)}$$
	- **Note**: From here it follows that $$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)} \iff P(A \cap B) = P(B|A) \ P(A)$$
	- This gives **Bayes' theorem**: $$P(A|B) = \frac{P(B|A) \ P(A)}{P(B)}$$
<!--ID: 1750164579892-->

## Bayes' theorem
<!--ID: 1750164579894-->

### Model

- A **model** is a mathematical formulation of an observed event.
<!--ID: 1750164579896-->

### Parameter

- A **parameter** is an input that influences a model.
<!--ID: 1750164579897-->

## Bayesian inference

- The crux of Bayesian inference is "updating" a prior belief in the face of new evidence. $$P(\theta | D) = \frac{P(D | \theta) \ P(\theta)}{P(D)}$$
- $P(\theta)$ is the **prior strength of the belief** without considering the evidence $D$.
- $P(D | \theta)$ is the likelihood of observing the result given the distribution for $\theta$
	- The probability of seeing the data $D$ as generated by a model with a parameter of $\theta$

The multiplication of $P(\theta)$ and $P(D | \theta)$ is **not** a multiplication between two numbers, but one between two **distributions**
<!--ID: 1750164579899-->
